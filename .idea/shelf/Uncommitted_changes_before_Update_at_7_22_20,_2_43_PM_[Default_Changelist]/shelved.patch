Index: ising_test_statistic_level_simulation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\nimport multiprocessing as mp\nimport os\n\nimport simulation_functions as sf\nimport generate_train_functions as gt\nimport hyperparameters as hp\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\npool = mp.Pool(processes=hp.process_number)\n\n# Set up\nmixutre_result_dict_name = f\"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}\"\nmixture_network_model_class_kwargs = {\"number_forward_layers\": hp.mixture_number_forward_layer,\n                                      \"input_dim\": hp.dim_z, \"hidden_dim\": hp.mixture_hidden_dim, \"output_dim\": 3}\nmixture_network_model_class_kwargs_vet = [mixture_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]\n\ntrue_result_dict_name = f\"ising_data_true_architecture\"\ntrue_network_model_class_kwargs = {\"number_forward_layers\": 1, \"input_dim\": hp.dim_z,\n                                   \"hidden_dim\": hp.hidden_1_out_dim, \"output_dim\": 3}\ntrue_network_model_class_kwargs_vet = [true_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]\n\ntrial_index_vet = [1, 10, 100]\n# Ising\n# Null data\nstart_time = time.time()\nsf.ising_bootstrap_loop(pool=pool, data_directory_name=\"ising_data\", scenario=\"null\",\n                        ising_simulation_result_dict_name=true_result_dict_name, result_dict_name=\"bootstrap\",\n                        trial_index_vet=trial_index_vet, network_model_class=gt.FullyConnectedNetwork,\n                        network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,\n                        number_of_bootstrap_samples=2, epoch_vet=hp.ising_epoch_vet)\n\n\nprint(f\"Gaussian process simulation under null Ising data takes {time.time() - start_time} seconds to finish.\")\n\n\"\"\"\nNot in use\n####################################\n# Simulate argmax Gaussian Process #\n####################################\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\n# Ising\n# Null data\nstart_time = time.time()\nsf.argmax_simulation_loop(pool=pool, trial_index_vet=np.arange(hp.number_of_trials), sample_size_vet=hp.sample_size_vet,\n                          scenario=\"null\", data_directory_name=\"ising_data\",\n                          ising_simulation_result_dict_name=true_result_dict_name,\n                          network_model_class=gt.FullyConnectedNetwork,\n                          network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,\n                          network_net_size=hp.network_net_size, number_of_nets=hp.number_of_nets,\n                          result_dict_name=\"gaussian_process\")\n\nprint(f\"Gaussian process simulation under null Ising data takes {time.time() - start_time} seconds to finish.\")\n\n\n# Alt\nstart_time = time.time()\nsf.argmax_simulation_loop(pool=pool, trial_index_vet=np.arange(hp.number_of_trials), sample_size_vet=[1000],\n                          scenario=\"alt\", data_directory_name=\"ising_data\",\n                          ising_simulation_result_dict_name=true_result_dict_name,\n                          network_model_class=gt.FullyConnectedNetwork,\n                          network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,\n                          network_net_size=hp.network_net_size, number_of_nets=hp.number_of_nets,\n                          result_dict_name=\"gaussian_process\")\n\nprint(f\"Gaussian process simulation under alternative Ising data takes {time.time() - start_time} seconds to finish.\")\n\"\"\"
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ising_test_statistic_level_simulation.py	(revision 59cbae34b8b6fe0a0702b4dfa80bac77a5913b53)
+++ ising_test_statistic_level_simulation.py	(date 1595106594479)
@@ -1,7 +1,7 @@
 import time
 import multiprocessing as mp
 import os
-
+import numpy as np
 import simulation_functions as sf
 import generate_train_functions as gt
 import hyperparameters as hp
@@ -11,7 +11,7 @@
 pool = mp.Pool(processes=hp.process_number)
 
 # Set up
-mixutre_result_dict_name = f"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}"
+mixture_result_dict_name = f"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}"
 mixture_network_model_class_kwargs = {"number_forward_layers": hp.mixture_number_forward_layer,
                                       "input_dim": hp.dim_z, "hidden_dim": hp.mixture_hidden_dim, "output_dim": 3}
 mixture_network_model_class_kwargs_vet = [mixture_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]
@@ -21,7 +21,7 @@
                                    "hidden_dim": hp.hidden_1_out_dim, "output_dim": 3}
 true_network_model_class_kwargs_vet = [true_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]
 
-trial_index_vet = [1, 10, 100]
+trial_index_vet = np.arange(2)
 # Ising
 # Null data
 start_time = time.time()
@@ -29,10 +29,11 @@
                         ising_simulation_result_dict_name=true_result_dict_name, result_dict_name="bootstrap",
                         trial_index_vet=trial_index_vet, network_model_class=gt.FullyConnectedNetwork,
                         network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,
-                        number_of_bootstrap_samples=2, epoch_vet=hp.ising_epoch_vet)
+                        number_of_bootstrap_samples=2, epoch_vet=hp.ising_epoch_vet,
+                        sample_size_vet=[50, 100])
 
 
-print(f"Gaussian process simulation under null Ising data takes {time.time() - start_time} seconds to finish.")
+print(f"Bootstrap simulation under null alt data takes {time.time() - start_time} seconds to finish.")
 
 """
 Not in use
Index: hyperparameters.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nimport os\n\n# Seed for random rate.\nseed_index=1\n\n###################\n# Data generation #\n###################\n# Dimension of the random_variables we condition on.\ndim_z = 3\n\n# Number of trials we simulate for each sample size\nnumber_of_trials = 1000\n\n# Sample size we simulate.\nsample_size_vet = np.array([50, 100, 500, 1000])\n\n# The radius we use for dividing z in to two groups under the mixture data scenario.\n# When dim_z = 3, the null_cut_off_radius will make P(x = -y) approximately 0.3252 and p(x = y) approximately 0.675.\n# When dim_z = 3, the alt_cut_off_radius will make P(norm(z) < alt_cut_off_radius) roughly 0.5.\nnull_cut_off_radius = 1.046 * np.sqrt(dim_z)\nalt_cut_off_radius = 0.8875 * np.sqrt(dim_z)\n\n\n##################\n# Nerual network #\n##################\n# Dimension of the hidden layer in the true network.\nhidden_1_out_dim = 3\n\n# Training epochs for samples sizes in the sample_size_vet\nising_epoch_vet = np.array([99, 129, 131, 91])\nmixture_epoch_vet = np.array([76, 71, 130, 93])\n\n# buffer size for Tensorflow dataset.\nbuffer_size = 1024\n\n# batch size for training.\nbatch_size = 100\n\n# learning rate for gradient descent.\nlearning_rate = 0.01\nlearning_rate_mixture = 0.01\n\n\n####################################\n# Architecture on the mixture data #\n####################################\nmixture_number_forward_layer = 1\nmixture_hidden_dim = 200\n\n\n############\n# Training #\n############\nnumber_of_test_samples_vet = [5, 10, 50, 100]\n\n\n################\n# Multiprocess #\n################\n# Number of process Pool function will run in parallel.\nprocess_number = os.cpu_count()\n\n\n###########################################\n# Sampling distribution of test statistic #\n###########################################\nnetwork_net_size = 500\nnumber_of_nets = 10**4\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- hyperparameters.py	(revision 59cbae34b8b6fe0a0702b4dfa80bac77a5913b53)
+++ hyperparameters.py	(date 1595106335414)
@@ -69,4 +69,4 @@
 ###########################################
 network_net_size = 500
 number_of_nets = 10**4
-
+number_of_bootstrap_samples = 1000
Index: simulation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\nimport multiprocessing as mp\nimport numpy as np\nimport tensorflow as tf\nimport os\n\nimport simulation_functions as sf\nimport generate_train_functions as gt\nimport hyperparameters as hp\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\npool = mp.Pool(processes=hp.process_number)\n\n##########################\n# Naive Chi squared test #\n##########################\n# Ising data\nsf.simulation_loop(pool=pool, simulation_method=sf.naive_chisq_method, scenario=\"null\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"naive_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nsf.simulation_loop(pool=pool, simulation_method=sf.naive_chisq_method, scenario=\"alt\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"naive_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\n# Naive Chisq simulation\nsf.simulation_loop(pool=pool, simulation_method=sf.naive_chisq_method, scenario=\"null\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"naive_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nsf.simulation_loop(pool=pool, simulation_method=sf.naive_chisq_method, scenario=\"alt\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"naive_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\n###############################\n# Stratified Chi squared test #\n###############################\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\n# Ising data\nsf.simulation_loop(pool=pool, simulation_method=sf.stratified_chisq_method, scenario=\"null\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"stratified_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nsf.simulation_loop(pool=pool, simulation_method=sf.stratified_chisq_method, scenario=\"alt\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"stratified_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\n# Mixture data\nsf.simulation_loop(pool=pool, simulation_method=sf.stratified_chisq_method, scenario=\"null\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"stratified_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials), cluster_number=2)\n\nsf.simulation_loop(pool=pool, simulation_method=sf.stratified_chisq_method, scenario=\"alt\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"stratified_chisq\",\n                   trial_index_vet=np.arange(hp.number_of_trials), cluster_number=2)\n\n###############\n# Ising Model #\n###############\n# Set up\nmixutre_result_dict_name = f\"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}\"\nmixture_network_model_class_kwargs = {\"number_forward_layers\": hp.mixture_number_forward_layer,\n                                      \"input_dim\": hp.dim_z, \"hidden_dim\": hp.mixture_hidden_dim, \"output_dim\": 3}\nmixture_network_model_class_kwargs_vet = [mixture_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]\n\ntrue_result_dict_name = f\"ising_data_true_architecture\"\ntrue_network_model_class_kwargs = {\"number_forward_layers\": 1, \"input_dim\": hp.dim_z,\n                                   \"hidden_dim\": hp.hidden_1_out_dim, \"output_dim\": 3}\ntrue_network_model_class_kwargs_vet = [true_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]\n\n# Mixture data\n# Alternative\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\nstart_time = time.time()\nsf.ising_simulation_loop(pool=pool, scenario=\"alt\", data_directory_name=\"mixture_data\",\n                         result_dict_name=mixutre_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),\n                         network_model_class=gt.FullyConnectedNetwork,\n                         network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,\n                         epoch_vet=hp.mixture_epoch_vet, learning_rate=hp.learning_rate_mixture,\n                         sample_size_vet=hp.sample_size_vet, number_of_test_samples_vet=hp.number_of_test_samples_vet)\n\nprint(\"Ising simulation under alternative mixture data takes %s seconds to finish.\" % (time.time() - start_time))\n\n# Null\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\nstart_time = time.time()\nsf.ising_simulation_loop(pool=pool, scenario=\"null\", data_directory_name=\"mixture_data\",\n                         result_dict_name=mixutre_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),\n                         network_model_class=gt.FullyConnectedNetwork,\n                         network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,\n                         epoch_vet=hp.mixture_epoch_vet, learning_rate=hp.learning_rate_mixture,\n                         sample_size_vet=hp.sample_size_vet, number_of_test_samples_vet=hp.number_of_test_samples_vet)\n\nprint(\"Ising simulation under null mixture data takes %s seconds to finish.\" % (time.time() - start_time))\n\n# Ising data\n# Alternative\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\nstart_time = time.time()\nsf.ising_simulation_loop(pool=pool, scenario=\"alt\", data_directory_name=\"ising_data\",\n                         result_dict_name=true_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),\n                         network_model_class=gt.FullyConnectedNetwork,\n                         network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,\n                         epoch_vet=hp.ising_epoch_vet, learning_rate=hp.learning_rate_mixture,\n                         sample_size_vet=hp.sample_size_vet, number_of_test_samples_vet=hp.number_of_test_samples_vet)\n\nprint(\"Ising simulation under null Ising data takes %s seconds to finish.\" % (time.time() - start_time))\n\n# Null\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\nstart_time = time.time()\nsf.ising_simulation_loop(pool=pool, scenario=\"null\", data_directory_name=\"ising_data\",\n                         result_dict_name=true_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),\n                         network_model_class=gt.FullyConnectedNetwork,\n                         network_model_class_kwargs_vet=true_network_model_class_kwargs_vet,\n                         epoch_vet=hp.ising_epoch_vet, learning_rate=hp.learning_rate_mixture,\n                         sample_size_vet=hp.sample_size_vet, number_of_test_samples_vet=hp.number_of_test_samples_vet)\n\nprint(\"Ising simulation under alternative Ising data takes %s seconds to finish.\" % (time.time() - start_time))\n\npool.close()\npool.join()\n\n########\n# CCIT #\n########\nnp.random.seed(hp.seed_index)\ntf.random.set_seed(hp.seed_index)\n\nprocess_number_ccit = 3\nccit_pool = mp.Pool(processes=process_number_ccit)\n\n# Ising data\nstart_time = time.time()\n\nsf.simulation_loop(pool=ccit_pool, simulation_method=sf.ccit_method, scenario=\"null\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"ccit\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nsf.simulation_loop(pool=ccit_pool, simulation_method=sf.ccit_method, scenario=\"alt\",\n                   data_directory_name=\"ising_data\", result_dict_name=\"ccit\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nprint(\"CCIT simulation takes %s seconds to finish.\" % (time.time() - start_time))\n\n# mixture data\nstart_time = time.time()\n\nsf.simulation_loop(pool=ccit_pool, simulation_method=sf.ccit_method, scenario=\"null\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"ccit\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nsf.simulation_loop(pool=ccit_pool, simulation_method=sf.ccit_method, scenario=\"alt\",\n                   data_directory_name=\"mixture_data\", result_dict_name=\"ccit\",\n                   trial_index_vet=np.arange(hp.number_of_trials))\n\nprint(\"CCIT simulation takes %s seconds to finish.\" % (time.time() - start_time))\n\nccit_pool.close()\nccit_pool.join()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- simulation.py	(revision 59cbae34b8b6fe0a0702b4dfa80bac77a5913b53)
+++ simulation.py	(date 1595106697383)
@@ -61,7 +61,7 @@
 # Ising Model #
 ###############
 # Set up
-mixutre_result_dict_name = f"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}"
+mixture_result_dict_name = f"mixture_data_{hp.mixture_number_forward_layer}_{hp.mixture_hidden_dim}"
 mixture_network_model_class_kwargs = {"number_forward_layers": hp.mixture_number_forward_layer,
                                       "input_dim": hp.dim_z, "hidden_dim": hp.mixture_hidden_dim, "output_dim": 3}
 mixture_network_model_class_kwargs_vet = [mixture_network_model_class_kwargs for _ in range(len(hp.sample_size_vet))]
@@ -78,7 +78,7 @@
 
 start_time = time.time()
 sf.ising_simulation_loop(pool=pool, scenario="alt", data_directory_name="mixture_data",
-                         result_dict_name=mixutre_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),
+                         result_dict_name=mixture_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),
                          network_model_class=gt.FullyConnectedNetwork,
                          network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,
                          epoch_vet=hp.mixture_epoch_vet, learning_rate=hp.learning_rate_mixture,
@@ -92,7 +92,7 @@
 
 start_time = time.time()
 sf.ising_simulation_loop(pool=pool, scenario="null", data_directory_name="mixture_data",
-                         result_dict_name=mixutre_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),
+                         result_dict_name=mixture_result_dict_name, trial_index_vet=np.arange(hp.number_of_trials),
                          network_model_class=gt.FullyConnectedNetwork,
                          network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,
                          epoch_vet=hp.mixture_epoch_vet, learning_rate=hp.learning_rate_mixture,
@@ -129,6 +129,40 @@
 
 print("Ising simulation under alternative Ising data takes %s seconds to finish." % (time.time() - start_time))
 
+
+###############################################
+# Bootstrap methods for Ising test statistic. #
+##############################################3
+# Mixture data
+# Null
+np.random.seed(hp.seed_index)
+tf.random.set_seed(hp.seed_index)
+
+start_time = time.time()
+sf.ising_bootstrap_loop(pool=pool, data_directory_name="mixture_data", scenario="null",
+                        ising_simulation_result_dict_name=mixture_result_dict_name, result_dict_name="bootstrap",
+                        trial_index_vet=np.arange(hp.number_of_trials), network_model_class=gt.FullyConnectedNetwork,
+                        network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,
+                        number_of_bootstrap_samples=hp.number_of_bootstrap_samples, epoch_vet=hp.mixture_epoch_vet)
+
+
+print(f"Bootstrap simulation under null data takes {time.time() - start_time} seconds to finish.")
+
+
+# Alternative
+np.random.seed(hp.seed_index)
+tf.random.set_seed(hp.seed_index)
+
+start_time = time.time()
+sf.ising_bootstrap_loop(pool=pool, data_directory_name="mixture_data", scenario="alt",
+                        ising_simulation_result_dict_name=mixture_result_dict_name, result_dict_name="bootstrap",
+                        trial_index_vet=np.arange(hp.number_of_trials), network_model_class=gt.FullyConnectedNetwork,
+                        network_model_class_kwargs_vet=mixture_network_model_class_kwargs_vet,
+                        number_of_bootstrap_samples=hp.number_of_bootstrap_samples, epoch_vet=hp.mixture_epoch_vet)
+
+
+print(f"Bootstrap simulation under alt data takes {time.time() - start_time} seconds to finish.")
+
 pool.close()
 pool.join()
 
Index: simulation_functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from functools import partial\nimport pickle\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\nfrom sklearn.cluster import KMeans\nfrom CCIT import CCIT\n\nimport generate_train_functions as gt\nimport hyperparameters as hp\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n\n####################\n# Simulation loops #\n####################\ndef simulation_loop(pool, simulation_method, scenario, data_directory_name, result_dict_name, trial_index_vet,\n                    sample_size_vet=hp.sample_size_vet, **kwargs):\n    \"\"\"\n    A wrapper function uses the multiprocessing Pool function to use simulation_method on multiple data in parallel. The\n    function will save the result dictionary in a pickle file under the path\n    ./results/ with file name {result_dict_name}_result_{scenario}_dict.p.\n\n    :param pool: A multiprocessing.pool.Pool instance.\n    :param simulation_method: A function which should one of the simulation method functions defined below except for\n        the ising_simulation_method function.\n    :param scenario: A string ('str' class) which is either \"null\" or \"alt\" indicating if the sample is simulated\n        under the null or alternative hypothesis.\n    :param data_directory_name: A string ('str' class) of the path towards the simulation data.\n    :param result_dict_name:  A string ('str' class) which we use to name the result dictionary as\n        {data_directory_name}/{result_dict_name}_{scenario}_result_dict.p.\n    :param trial_index_vet: An array of integers which contains the trial indices of data used.\n    :param sample_size_vet: A python list of integers. It contains all the sample size we simulated.\n    :param kwargs: Additional keywords arguments to pass into the simulation_method if necessary.\n\n    :return:\n        None.\n    \"\"\"\n    result_dict = dict()\n\n    for sample_size in sample_size_vet:\n        pool_result_vet = pool.map(partial(simulation_method, sample_size=sample_size, scenario=scenario,\n                                           data_directory_name=data_directory_name, **kwargs), trial_index_vet)\n\n        result_dict[sample_size] = dict(pool_result_vet)\n\n        print(f\"{result_dict_name}, {scenario}, {sample_size} finished\")\n\n    with open(f\"./results/result_dict/{data_directory_name}/{result_dict_name}_{scenario}_result_dict.p\", \"wb\") as fp:\n        pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef ising_simulation_loop(pool, scenario, data_directory_name, result_dict_name, trial_index_vet, network_model_class,\n                          network_model_class_kwargs_vet, epoch_vet, learning_rate, sample_size_vet=hp.sample_size_vet,\n                          number_of_test_samples_vet=hp.number_of_test_samples_vet):\n    \"\"\"\n    A wrapper function uses the multiprocessing Pool function to use tbe ising_simulation_method on multiple data in\n    parallel. The function will save the result dictionary in a pickle file under the path\n    ./results/{data_directory_name}/ with file name {result_dict_name}_{scenario}_result_dict.p.\n\n    :param pool: A multiprocessing.pool.Pool instance.\n    :param scenario: A string ('str' class) which is either \"null\" or \"alt\" indicating if the sample is simulated\n        under the null or alternative hypothesis.\n    :param data_directory_name: A string ('str' class) of the path towards the simulation data.\n    :param result_dict_name:  A string ('str' class) which we use to name the result dictionary as\n        {data_directory_name}/{result_dict_name}_{scenario}_result_dict.p.\n    :param trial_index_vet: An array of integers which contains the trial indices of data used.\n    :param network_model_class: A subclass of tf.keras.Model with output dimension 3. An instance of the class is the\n        neural network to fit on the data.\n    :param network_model_class_kwargs_vet: An array of dictionaries. The array should have the same length as the\n        sample_size_vet does. Each dictionary contains keyword arguments to create an instance of the\n        network_model_class.\n    :param epoch_vet: An array of integers. The array should have the same length as the sample_size_vet does.\n        Each entry specifies the number of epochs we train the network on data with the corresponding sample size.\n    :param learning_rate: A scalar which is a (hyper)parameter in the tf.keras.optimizers.Adam function.\n    :param sample_size_vet: A python list of integers. It contains all the sample size we simulated.\n    :param number_of_test_samples_vet: An array of integers which contains the sample size of data used.\n\n    :return:\n        None\n    \"\"\"\n    result_dict = dict()\n\n    for sample_size, number_of_test_samples, epoch, network_model_class_kwargs in zip(sample_size_vet,\n                                                                                      number_of_test_samples_vet,\n                                                                                      epoch_vet,\n                                                                                      network_model_class_kwargs_vet):\n        pool_result_vet = pool.map(partial(ising_simulation_method, sample_size=sample_size, scenario=scenario,\n                                           data_directory_name=data_directory_name, epoch=epoch,\n                                           network_model_class=network_model_class,\n                                           number_of_test_samples=number_of_test_samples, learning_rate=learning_rate,\n                                           network_model_class_kwargs=network_model_class_kwargs), trial_index_vet)\n\n        result_dict[sample_size] = dict(pool_result_vet)\n\n        print(f\"{result_dict_name}, {scenario}, sample size: {sample_size} finished\")\n\n    with open(f\"./results/result_dict/{data_directory_name}/{result_dict_name}_{scenario}_result_dict.p\", \"wb\") as fp:\n        pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef ising_bootstrap_loop(pool, scenario, data_directory_name, ising_simulation_result_dict_name, result_dict_name,\n                         trial_index_vet, network_model_class,\n                         network_model_class_kwargs_vet, number_of_bootstrap_samples, epoch_vet,\n                         learning_rate=hp.learning_rate,\n                         sample_size_vet=hp.sample_size_vet):\n    result_dict = {}\n    for sample_size, network_model_class_kwargs, epoch in zip(sample_size_vet,\n                                                              network_model_class_kwargs_vet, epoch_vet):\n        sample_size_result_dict = {}\n        for trial_index in trial_index_vet:\n            sample_size_result_dict[trial_index] = \\\n                ising_bootstrap_method(pool=pool, trial_index=trial_index, sample_size=sample_size, scenario=scenario,\n                                       data_directory_name=data_directory_name,\n                                       ising_simulation_result_dict_name=ising_simulation_result_dict_name,\n                                       network_model_class=network_model_class,\n                                       network_model_class_kwargs=network_model_class_kwargs,\n                                       number_of_bootstrap_samples=number_of_bootstrap_samples, epoch=epoch,\n                                       learning_rate=learning_rate)\n        result_dict[sample_size] = sample_size_result_dict\n\n    with open(f\"./results/result_dict/{data_directory_name}/{result_dict_name}_{scenario}_result_dict.p\", \"wb\") as fp:\n        pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Not in use.\n# def argmax_simulation_loop(pool, trial_index_vet, sample_size_vet, scenario, data_directory_name,\n#                            ising_simulation_result_dict_name, result_dict_name, network_model_class,\n#                            network_model_class_kwargs_vet, network_net_size, number_of_nets):\n#     \"\"\"\n#\n#     :param result_dict_name:\n#     :param pool:\n#     :param trial_index_vet:\n#     :param sample_size_vet:\n#     :param scenario:\n#     :param data_directory_name:\n#     :param ising_simulation_result_dict_name:\n#     :param network_model_class:\n#     :param network_model_class_kwargs_vet:\n#     :param network_net_size:\n#     :param number_of_nets:\n#     :return:\n#     \"\"\"\n#     result_dict = {}\n#     for sample_size, network_model_class_kwargs in zip(sample_size_vet, network_model_class_kwargs_vet):\n#         sample_size_result_dict = {}\n#         for trial_index in trial_index_vet:\n#             sample_size_result_dict[trial_index] = \\\n#                 argmax_gaussian_process_simulation_method(pool=pool, trial_index=trial_index, sample_size=sample_size,\n#                                                           scenario=scenario, data_directory_name=data_directory_name,\n#                                                           ising_simulation_result_dict_name=ising_simulation_result_dict_name,\n#                                                           network_model_class=network_model_class,\n#                                                           network_model_class_kwargs=network_model_class_kwargs,\n#                                                           network_net_size=network_net_size,\n#                                                           number_of_nets=number_of_nets)\n#         result_dict[sample_size] = sample_size_result_dict\n#\n#     with open(f\"./results/result_dict/{data_directory_name}/{result_dict_name}_{scenario}_result_dict.p\", \"wb\") as fp:\n#         pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n#\n#     return result_dict\n\n\n#####################\n# Wrapper functions #\n#####################\n# Ising simulation\ndef ising_simulation_method(trial_index, sample_size, scenario, data_directory_name, epoch, network_model_class,\n                            network_model_class_kwargs, number_of_test_samples, learning_rate=hp.learning_rate):\n    \"\"\"\n    This function fit a Neural Ising model to compute the test statistic and record the indices of test samples\n    on {trial_index}th trial with sample size {sample_size} under the {scenario} hypothesis.\n    The tuple returned return will be used to create a dictionary.\n\n    :param data_directory_name: It should either be \"ising_data\" or \"mixture_data\" depending on if the data is generated\n        under the Ising or mixture model.\n    :param trial_index: An integer indicating the data is the {trial_index}th trial of sample size {sample_size}.\n    :param sample_size: An integer indicating the sample size of the data.\n    :param scenario: It should be either \"null\" or \"alt\" depending on if the data is generated under the null or\n        alternative.\n    param data_directory_name: It should either be \"ising_data\" or \"mixture_data\" depending on if the data is generated\n        under the Ising or mixture model.\n    :param epoch: An integer indicating the number of training epoch when training the neural network.\n    :param network_model_class: A subclass of tf.keras.Model with output dimension 3. An instance of the class is the\n        neural network to fit on the data.\n    :param network_model_class_kwargs: Keyword arguments to be passed in to the constructor of the network_model_class.\n    :param number_of_test_samples: An integer which is the number of samples used as the test data.\n    :param learning_rate: A scalar which is a (hyper)parameter in the tf.keras.optimizers.Adam function.\n\n    :return:\n        A tuple (trial_index, result_vet). The result_vet is the output of the class method train_compute_test_statistic\n        of the gt.NetworkTrainingTuning class.\n    \"\"\"\n    x_y_mat = np.loadtxt(f\"./data/{data_directory_name}/{scenario}/x_y_mat_{sample_size}_{trial_index}.txt\",\n                         dtype=np.float32)\n    z_mat = np.loadtxt(f\"./data/{data_directory_name}/z_mat/z_mat_{sample_size}_{trial_index}.txt\", dtype=np.float32)\n\n    training_tuning_instance = gt.NetworkTrainingTuning(z_mat=z_mat, x_y_mat=x_y_mat,\n                                                        network_model_class=network_model_class,\n                                                        network_model_class_kwargs=network_model_class_kwargs,\n                                                        learning_rate=learning_rate, epoch=epoch)\n\n    result_dict = training_tuning_instance.train_compute_test_statistic(print_loss_boolean=False,\n                                                                        number_of_test_samples=number_of_test_samples)\n\n    print(f\"Scenario: {scenario} Sample size: {sample_size} trial: {trial_index} is done.\")\n\n    return (trial_index, result_dict)\n\n\ndef ising_bootstrap_method(pool, trial_index, sample_size, scenario, data_directory_name,\n                           ising_simulation_result_dict_name, network_model_class,\n                           network_model_class_kwargs, number_of_bootstrap_samples, epoch,\n                           learning_rate=hp.learning_rate):\n    \"\"\"\n\n    :param pool:\n    :param trial_index:\n    :param sample_size:\n    :param scenario:\n    :param data_directory_name:\n    :param ising_simulation_result_dict_name:\n    :param network_model_class:\n    :param network_model_class_kwargs:\n    :param number_of_bootstrap_samples:\n    :param epoch:\n    :param learning_rate:\n    :return:\n    \"\"\"\n    with open(f'results/result_dict/{data_directory_name}/{ising_simulation_result_dict_name}_{scenario}_result_'\n              f'dict.p', 'rb') as fp:\n        ising_simulation_loop_result_dict = pickle.load(fp)\n\n    trial_test_statistic = ising_simulation_loop_result_dict[sample_size][trial_index][\"test_statistic\"]\n    test_indices_vet = ising_simulation_loop_result_dict[sample_size][trial_index][\"test_indices_vet\"]\n\n    x_y_mat = np.loadtxt(f\"./data/{data_directory_name}/{scenario}/x_y_mat_{sample_size}_{trial_index}.txt\",\n                         dtype=np.float32)\n    z_mat = np.loadtxt(f\"./data/{data_directory_name}/z_mat/z_mat_{sample_size}_{trial_index}.txt\", dtype=np.float32)\n\n    training_tuning_instance = gt.NetworkTrainingTuning(z_mat=z_mat, x_y_mat=x_y_mat,\n                                                        network_model_class=network_model_class,\n                                                        network_model_class_kwargs=network_model_class_kwargs,\n                                                        learning_rate=learning_rate, epoch=epoch)\n\n    bootstrap_sample_vet = training_tuning_instance.bootstrap(pool=pool, test_indices_vet=test_indices_vet,\n                                                              number_of_bootstrap_samples=number_of_bootstrap_samples)\n\n    p_value =sum(trial_test_statistic < bootstrap_sample_vet) / number_of_bootstrap_samples\n    print(f\"Scenario: {scenario} Sample size: {sample_size} trial: {trial_index} is done. p-value: {p_value}\")\n\n    return p_value\n\n\n\n# not in use.\n# def argmax_gaussian_process_simulation_method(pool, trial_index, sample_size, scenario, data_directory_name,\n#                                               ising_simulation_result_dict_name, network_model_class,\n#                                               network_model_class_kwargs, network_net_size, number_of_nets):\n#     \"\"\"\n#\n#     :param pool:\n#     :param trial_index:\n#     :param sample_size:\n#     :param scenario:\n#     :param data_directory_name:\n#     :param ising_simulation_result_dict_name:\n#     :param network_model_class:\n#     :param network_model_class_kwargs:\n#     :param network_net_size:\n#     :param number_of_nets:\n#     :return:\n#     \"\"\"\n#     with open(f'results/result_dict/{data_directory_name}/{ising_simulation_result_dict_name}_{scenario}_result_'\n#               f'dict.p', 'rb') as fp:\n#         ising_simulation_loop_result_dict = pickle.load(fp)\n#\n#     trial_test_statistic = ising_simulation_loop_result_dict[sample_size][trial_index][\"test_statistic\"]\n#     test_indices_vet = ising_simulation_loop_result_dict[sample_size][trial_index][\"test_indices_vet\"]\n#\n#     z_mat = np.loadtxt(f\"./data/{data_directory_name}/z_mat/z_mat_{sample_size}_{trial_index}.txt\", dtype=np.float32)\n#     test_z_mat = z_mat[test_indices_vet, :]\n#\n#     test_statistic_sample_vet = \\\n#         gt.argmax_gaussian_process_one_trial(pool=pool, z_mat=test_z_mat, network_model_class=network_model_class,\n#                                              network_model_class_kwargs=network_model_class_kwargs,\n#                                              network_net_size=network_net_size, number_of_nets=number_of_nets)\n#\n#     p_value = sum(trial_test_statistic < test_statistic_sample_vet) / number_of_nets\n#     print(f\"Scenario: {scenario} Sample size: {sample_size} trial: {trial_index} is done. p-value: {p_value}\")\n#     return p_value\n\n\n# Naive Chisq\ndef chi_squared_test(x_y_mat):\n    \"\"\"\n    Perform unconditional Chi-squared test.\n\n    :param x_y_mat: A 2-d numpy array with columns corresponding to x and y.\n\n    :return:\n    result_vet: A length 2 tuple. result_vet[0] is the test statistic and result_vet[1] is the p-value.\n    \"\"\"\n    x_vet = x_y_mat[:, 0]\n    y_vet = x_y_mat[:, 1]\n    contingency_table = pd.crosstab(x_vet, y_vet, rownames=\"x\", colnames=\"y\")\n\n    result_vet = chi2_contingency(contingency_table)[0:2]\n    return result_vet\n\n\ndef naive_chisq_method(trial_index, scenario, data_directory_name, sample_size):\n    \"\"\"\n    A wrapper function for the multiprocessing Pool function. It will be passed into the partial function.\n    The pool function will run iterations in parallel given a sample size and a scenario.\n    This function perform Chi squared test on {trial_index}th trial with sample size\n    {sample_size} under the {scenario} hypothesis.\n    The return will be used to create a dictionary.\n\n    :param trial_index: An integer indicating {trial_index}th trial among simulations under sample size {sample_size}.\n    :param scenario: A string ('str' class) which is either \"null\" or \"alt\" indicating if the sample is simulated\n    under the null or alternative hypothesis.\n    :param data_directory_name: A string ('str' class) of the path towards the simulation data.\n    :param sample_size: An integer.\n\n    :return:\n        A tuple (trial_index, result_vet). result_vet is the return of the chi_squared_test function.\n    \"\"\"\n    x_y_mat = np.loadtxt(f\"./data/{data_directory_name}/{scenario}/x_y_mat_{sample_size}_{trial_index}.txt\")\n    result_vet = chi_squared_test(x_y_mat)\n    return (trial_index, result_vet)\n\n\n# Stratified Chisq\ndef stratify_x_y_mat(x_y_mat, z_mat, cluster_number=2):\n    \"\"\"\n    Cluster data into {cluster_number} of clusters.\n\n    :param x_y_mat: An n x 2 numpy array. Each row is the response of the ith observation. First column corresponds to\n        x.\n    :param z_mat: A 2D numpy array. Each row is a sample\n    :param cluster_number: An integer which is the number of clusters to form by the KMeans function.\n\n    :return:\n        z_mat_vet: A python list of length {cluster_number}. Each element is a 2D numpy array of a data cluster.\n    \"\"\"\n    kmeans_model = KMeans(n_clusters=cluster_number, random_state=0)\n    kmeans_result_vet = kmeans_model.fit(z_mat).labels_\n    x_y_mat_vet = [x_y_mat[kmeans_result_vet == i, :] for i in range(cluster_number)]\n\n    return x_y_mat_vet\n\n\ndef stratified_chi_squared_test(x_y_mat_vet):\n    \"\"\"\n    Compute sum of the Chi squared statistic on each strata.\n\n    :param x_y_mat_vet: A python list which is the output of the stratify_x_y_mat function.\n\n    :return:\n        A numeric which is the sum of of the chi-squared statistic on each stratum.\n    \"\"\"\n    chi_square_statistic_vet = np.zeros(len(x_y_mat_vet))\n    for iteration, x_y_mat in enumerate(x_y_mat_vet):\n        chi_square_statistic_vet[iteration] = chi_squared_test(x_y_mat)[0]\n\n    return sum(chi_square_statistic_vet)\n\n\ndef stratified_chisq_method(trial_index, scenario, data_directory_name, sample_size, cluster_number=2):\n    \"\"\"\n    A wrapper function for the multiprocessing Pool function. It will be passed into the partial function.\n    The pool function will run iteration in parallel given a sample size and a scenario.\n    This function perform stratified Chi squared test on {trial_index}th trial with sample size\n    {sample_size} under the {scenario} hypothesis.\n    The return will be used to create a dictionary.\n\n    :param trial_index: An integer indicating {trial_index}th trial among simulations under sample size {sample_size}.\n    :param scenario: A string ('str' class) which is either \"null\" or \"alt\" indicating if the sample is simulated\n        under the null or alternative hypothesis.\n    :param sample_size: An integer.\n    :param data_directory_name: A string ('str' class) of the path towards the simulation data.\n\n    :return:\n        A tuple (trial_index, result_vet). result_vet is the return of the stratified_chi_squared_test function.\n    \"\"\"\n    x_y_mat = np.loadtxt(f\"./data/{data_directory_name}/{scenario}/x_y_mat_{sample_size}_{trial_index}.txt\")\n    z_mat = np.loadtxt(f\"./data/{data_directory_name}/z_mat/z_mat_{sample_size}_{trial_index}.txt\", dtype=np.float32)\n\n    x_y_mat_vet = stratify_x_y_mat(x_y_mat=x_y_mat, z_mat=z_mat, cluster_number=cluster_number)\n    test_statistic = stratified_chi_squared_test(x_y_mat_vet)\n\n    return (trial_index, test_statistic)\n\n\n# CCIT\ndef process_x_y_mat(x_y_mat):\n    \"\"\"\n    Process the data so that the input can be fed in to the CCIT.CCIT function.\n\n    :param x_y_mat: An n x 2 numpy array. Each row is the response of the ith observation.\n    First column corresponds to x.\n\n    :return:\n        1. x: An n x 1 numpy array corresponding to X.\n        2. y: An n x 1 numpy array corresponding to Y.\n    \"\"\"\n    x = x_y_mat[:, 0][:, np.newaxis]\n    y = x_y_mat[:, 1][:, np.newaxis]\n    return x, y\n\n\ndef ccit_method(trial_index, scenario, data_directory_name, sample_size, **kwargs):\n    \"\"\"\n    A wrapper function for the multiprocessing Pool function. It will be passed into the partial function.\n    The pool function will run iteration in parallel given a sample size and a scenario.\n    This function perform the model powered conditional independence test proposed by  on {trial_index}th trial with\n    sample size {sample_size} under the {scenario} hypothesis.\n    The return will be used to create a dictionary.\n\n    :param trial_index: An integer indicating {trial_index}th trial among simulations under sample size {sample_size}.\n    :param scenario: A string ('str' class) which is either \"null\" or \"alt\" indicating if the sample is simulated\n        under the null or alternative hypothesis.\n    :param data_directory_name: A string ('str' class) of the path towards the simulation data.\n    :param sample_size: An integer.\n    :param kwargs: Arguments for the CCIT.CCIT functions.\n\n    :return:\n        A tuple (trial_index, result_vet). result_vet is the return of the CCIT.CCIT function.\n    \"\"\"\n    x_y_mat = np.loadtxt(f\"./data/{data_directory_name}/{scenario}/x_y_mat_{sample_size}_{trial_index}.txt\")\n    x_array, y_array = process_x_y_mat(x_y_mat)\n    z_mat = np.loadtxt(f\"./data/{data_directory_name}/z_mat/z_mat_{sample_size}_{trial_index}.txt\", dtype=np.float32)\n\n    p_value = CCIT.CCIT(x_array, y_array, z_mat, **kwargs)\n\n    print(f\"{scenario}: Sample size {sample_size} simulation {trial_index} is done.\")\n\n    return (trial_index, p_value)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- simulation_functions.py	(revision 59cbae34b8b6fe0a0702b4dfa80bac77a5913b53)
+++ simulation_functions.py	(date 1595106051625)
@@ -111,7 +111,7 @@
                                                               network_model_class_kwargs_vet, epoch_vet):
         sample_size_result_dict = {}
         for trial_index in trial_index_vet:
-            sample_size_result_dict[trial_index] = \
+            trial_result_dict = \
                 ising_bootstrap_method(pool=pool, trial_index=trial_index, sample_size=sample_size, scenario=scenario,
                                        data_directory_name=data_directory_name,
                                        ising_simulation_result_dict_name=ising_simulation_result_dict_name,
@@ -119,11 +119,14 @@
                                        network_model_class_kwargs=network_model_class_kwargs,
                                        number_of_bootstrap_samples=number_of_bootstrap_samples, epoch=epoch,
                                        learning_rate=learning_rate)
+            sample_size_result_dict[trial_index] = trial_result_dict
+
         result_dict[sample_size] = sample_size_result_dict
 
     with open(f"./results/result_dict/{data_directory_name}/{result_dict_name}_{scenario}_result_dict.p", "wb") as fp:
         pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)
 
+
 # Not in use.
 # def argmax_simulation_loop(pool, trial_index_vet, sample_size_vet, scenario, data_directory_name,
 #                            ising_simulation_result_dict_name, result_dict_name, network_model_class,
@@ -245,14 +248,14 @@
                                                         network_model_class_kwargs=network_model_class_kwargs,
                                                         learning_rate=learning_rate, epoch=epoch)
 
-    bootstrap_sample_vet = training_tuning_instance.bootstrap(pool=pool, test_indices_vet=test_indices_vet,
-                                                              number_of_bootstrap_samples=number_of_bootstrap_samples)
+    test_statistic_sample = training_tuning_instance.bootstrap(pool=pool, test_indices_vet=test_indices_vet,
+                                                               number_of_bootstrap_samples=number_of_bootstrap_samples)
 
-    p_value =sum(trial_test_statistic < bootstrap_sample_vet) / number_of_bootstrap_samples
+    p_value = sum(trial_test_statistic < test_statistic_sample) / number_of_bootstrap_samples
     print(f"Scenario: {scenario} Sample size: {sample_size} trial: {trial_index} is done. p-value: {p_value}")
 
-    return p_value
-
+    result_dict = {"p_value": p_value, "test_statistic_sample": test_statistic_sample}
+    return result_dict
 
 
 # not in use.
